{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f946a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install datasets accelerate -q\n",
    "!pip install -U transformers\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# 1. Load dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"/kaggle/input/dataset-chatbot-heart/data_chatbot_training.jsonl\"})\n",
    "\n",
    "# 2. Load tokenizer & model GPT-2\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 mặc định không có token padding, cần thêm:\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# 3. Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# 4. Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # GPT-2 là causal LM, không dùng masked LM\n",
    ")\n",
    "\n",
    "# 5. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-heart-chatbot\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",    # nơi lưu log tensorboard\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "\n",
    "# 6. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 7. Train (sẽ hiển thị log chi tiết trong output)\n",
    "trainer.train()\n",
    "\n",
    "# 8. Save model\n",
    "trainer.save_model(\"./gpt2-heart-chatbot\")\n",
    "tokenizer.save_pretrained(\"./gpt2-heart-chatbot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c404e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.7/47.7 MB 38.4 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 4.7 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 105.4 MB/s eta 0:00:0000:010:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 76.8 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 45.2 MB/s eta 0:00:00\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.5 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 5.3 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 31.1 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 13.6 MB/s eta 0:00:0000:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 2.0 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 87.7 MB/s eta 0:00:00:00:0100:01\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
    "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
    "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
    "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
    "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
    "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
    "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
    "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
    "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
    "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
    "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
    "Collecting transformers\n",
    "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
    "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.0/44.0 kB 1.5 MB/s eta 0:00:00\n",
    "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\n",
    "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
    "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
    "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
    "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
    "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\n",
    "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\n",
    "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
    "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
    "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
    "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
    "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
    "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
    "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
    "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
    "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
    "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
    "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
    "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
    "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
    "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
    "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
    "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
    "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
    "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
    "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
    "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
    "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
    "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
    "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/12.0 MB 105.5 MB/s eta 0:00:0000:01:01\n",
    "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 566.1/566.1 kB 27.6 MB/s eta 0:00:00\n",
    "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 82.9 MB/s eta 0:00:00:00:01\n",
    "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
    "  Attempting uninstall: huggingface-hub\n",
    "    Found existing installation: huggingface-hub 1.0.0rc2\n",
    "    Uninstalling huggingface-hub-1.0.0rc2:\n",
    "      Successfully uninstalled huggingface-hub-1.0.0rc2\n",
    "  Attempting uninstall: tokenizers\n",
    "    Found existing installation: tokenizers 0.21.2\n",
    "    Uninstalling tokenizers-0.21.2:\n",
    "      Successfully uninstalled tokenizers-0.21.2\n",
    "  Attempting uninstall: transformers\n",
    "    Found existing installation: transformers 4.53.3\n",
    "    Uninstalling transformers-4.53.3:\n",
    "      Successfully uninstalled transformers-4.53.3\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
    "Successfully installed huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.57.1\n",
    "2025-11-03 12:59:17.576489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
    "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
    "E0000 00:00:1762174757.962581      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
    "E0000 00:00:1762174758.069252      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
    "Loading widget...\n",
    "Loading widget...\n",
    "Loading widget...\n",
    "Loading widget...\n",
    "Loading widget...\n",
    "Loading widget...\n",
    "Loading widget...\n",
    "Loading widget...\n",
    "Loading widget...\n",
    "/tmp/ipykernel_37/1704901427.py:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
    "  trainer = Trainer()\n",
    "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
    "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
    "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
    " [1260/1260 06:19, Epoch 10/10]\n",
    "Step\tTraining Loss\n",
    "10\t3.048400\n",
    "20\t2.058100\n",
    "30\t1.873000\n",
    "40\t1.809600\n",
    "50\t1.783700\n",
    "60\t1.723600\n",
    "70\t1.656900\n",
    "80\t1.650800\n",
    "90\t1.544300\n",
    "100\t1.642200\n",
    "110\t1.562900\n",
    "120\t1.506300\n",
    "130\t1.374900\n",
    "140\t1.401100\n",
    "150\t1.355300\n",
    "160\t1.256700\n",
    "170\t1.333400\n",
    "180\t1.345100\n",
    "190\t1.364200\n",
    "200\t1.415200\n",
    "210\t1.310300\n",
    "220\t1.289600\n",
    "230\t1.302900\n",
    "240\t1.170800\n",
    "250\t1.225200\n",
    "260\t1.060600\n",
    "270\t1.100900\n",
    "280\t1.092800\n",
    "290\t1.122300\n",
    "300\t1.180300\n",
    "310\t1.167800\n",
    "320\t1.164600\n",
    "330\t1.164800\n",
    "340\t1.213700\n",
    "350\t1.040300\n",
    "360\t1.155700\n",
    "370\t1.054400\n",
    "380\t1.174300\n",
    "390\t1.006700\n",
    "400\t1.010000\n",
    "410\t0.983000\n",
    "420\t0.980000\n",
    "430\t1.057700\n",
    "440\t1.007100\n",
    "450\t1.041600\n",
    "460\t1.021600\n",
    "470\t0.994100\n",
    "480\t0.988000\n",
    "490\t1.001900\n",
    "500\t1.054700\n",
    "510\t0.881700\n",
    "520\t0.904200\n",
    "530\t0.889300\n",
    "540\t0.978500\n",
    "550\t0.885600\n",
    "560\t0.876300\n",
    "570\t0.903700\n",
    "580\t0.906700\n",
    "590\t0.956600\n",
    "600\t0.953900\n",
    "610\t0.916200\n",
    "620\t0.912000\n",
    "630\t0.863400\n",
    "640\t0.761700\n",
    "650\t0.882800\n",
    "660\t0.849500\n",
    "670\t0.867500\n",
    "680\t0.834400\n",
    "690\t0.815500\n",
    "700\t0.808900\n",
    "710\t0.873300\n",
    "720\t0.801000\n",
    "730\t0.865300\n",
    "740\t0.883000\n",
    "750\t0.858600\n",
    "760\t0.769100\n",
    "770\t0.751600\n",
    "780\t0.762200\n",
    "790\t0.726800\n",
    "800\t0.773000\n",
    "810\t0.801800\n",
    "820\t0.774700\n",
    "830\t0.752500\n",
    "840\t0.758900\n",
    "850\t0.831300\n",
    "860\t0.785700\n",
    "870\t0.793600\n",
    "880\t0.777100\n",
    "890\t0.721500\n",
    "900\t0.710800\n",
    "910\t0.711900\n",
    "920\t0.715500\n",
    "930\t0.703500\n",
    "940\t0.764800\n",
    "950\t0.712300\n",
    "960\t0.789200\n",
    "970\t0.697700\n",
    "980\t0.749900\n",
    "990\t0.740200\n",
    "1000\t0.745600\n",
    "1010\t0.726300\n",
    "1020\t0.672900\n",
    "1030\t0.696400\n",
    "1040\t0.727300\n",
    "1050\t0.704500\n",
    "1060\t0.693300\n",
    "1070\t0.719700\n",
    "1080\t0.716400\n",
    "1090\t0.701800\n",
    "1100\t0.675000\n",
    "1110\t0.686200\n",
    "1120\t0.724400\n",
    "1130\t0.702700\n",
    "1140\t0.625300\n",
    "1150\t0.647800\n",
    "1160\t0.662500\n",
    "1170\t0.648800\n",
    "1180\t0.687000\n",
    "1190\t0.666800\n",
    "1200\t0.673800\n",
    "1210\t0.684300\n",
    "1220\t0.629700\n",
    "1230\t0.753100\n",
    "1240\t0.676200\n",
    "1250\t0.674600\n",
    "1260\t0.668200\n",
    "('./gpt2-heart-chatbot/tokenizer_config.json',\n",
    " './gpt2-heart-chatbot/special_tokens_map.json',\n",
    " './gpt2-heart-chatbot/vocab.json',\n",
    " './gpt2-heart-chatbot/merges.txt',\n",
    " './gpt2-heart-chatbot/added_tokens.json',\n",
    " './gpt2-heart-chatbot/tokenizer.json')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
