{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3d6dbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80ccd91c",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59a6f1",
   "metadata": {},
   "source": [
    "!pip install transformers datasets accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d487c5",
   "metadata": {},
   "source": [
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 566.1/566.1 kB 7.3 MB/s eta 0:00:0000:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.7/47.7 MB 37.5 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 4.7 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 86.8 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 55.7 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 42.1 MB/s eta 0:00:00\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.6 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 8.0 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 28.6 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 13.4 MB/s eta 0:00:0000:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 8.2 MB/s eta 0:00:00:00:0100:01\n",
    "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 71.2 MB/s eta 0:00:00:00:0100:01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52876e86",
   "metadata": {},
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e57538",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "csv_path = \"/kaggle/input/dataset-training-chatbot/train_data_chatbot.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Columns found:\", df.columns.tolist())\n",
    "\n",
    "if not {'short_question', 'short_answer'}.issubset(df.columns):\n",
    "    raise ValueError(\"CSV cần có 2 cột: 'short_question' và 'short_answer'\")\n",
    "\n",
    "df[\"text\"] = \"User: \" + df[\"short_question\"].astype(str) + \" Bot: \" + df[\"short_answer\"].astype(str)\n",
    "df = df.dropna(subset=[\"text\"])\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"text\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"Dataset size — Train: {len(dataset['train'])}, Test: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a107a",
   "metadata": {},
   "source": [
    "Columns found: ['short_question', 'short_answer', 'tags', 'label']\n",
    "Dataset size — Train: 42842, Test: 4761"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e24cf5",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4c7b5",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokens = tokenizer(\n",
    "        batch[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62626b8f",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d82b10",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e54a83",
   "metadata": {},
   "source": [
    "tokenizer_config.json: 100%\n",
    " 614/614 [00:00<00:00, 66.8kB/s]\n",
    "vocab.json: \n",
    " 1.04M/? [00:00<00:00, 24.1MB/s]\n",
    "merges.txt: \n",
    " 456k/? [00:00<00:00, 36.0MB/s]\n",
    "Map: 100%\n",
    " 42842/42842 [00:10<00:00, 4386.00 examples/s]\n",
    "Map: 100%\n",
    " 4761/4761 [00:01<00:00, 4490.07 examples/s]\n",
    "config.json: 100%\n",
    " 641/641 [00:00<00:00, 91.3kB/s]\n",
    "model.safetensors: 100%\n",
    " 351M/351M [00:01<00:00, 273MB/s]\n",
    "generation_config.json: 100%\n",
    " 124/124 [00:00<00:00, 15.5kB/s]\n",
    "/tmp/ipykernel_37/372943805.py:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
    "  trainer = Trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9fd3e",
   "metadata": {},
   "source": [
    "Training arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35549e",
   "metadata": {},
   "source": [
    "Bắt đầu huấn luyện..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052576e",
   "metadata": {},
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486604ab",
   "metadata": {},
   "source": [
    "Trainer setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1b1ae",
   "metadata": {},
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "<!-- print(\"Bắt đầu huấn luyện...\")\n",
    "trainer.train()\n",
    "print(\"Huấn luyện hoàn tất.\") -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2997d37",
   "metadata": {},
   "source": [
    "[5356/5356 1:17:24, Epoch 1/1]\n",
    "Step\tTraining Loss\tValidation Loss\n",
    "100\t1.670300\t3.117189\n",
    "200\t1.516600\t2.977963\n",
    "300\t1.565900\t2.926508\n",
    "400\t1.570700\t2.887151\n",
    "500\t1.503200\t2.855027\n",
    "600\t1.532700\t2.841193\n",
    "700\t1.355200\t2.817111\n",
    "800\t1.489900\t2.797966\n",
    "900\t1.500100\t2.782957\n",
    "1000\t1.445900\t2.771570\n",
    "1100\t1.354800\t2.759558\n",
    "1200\t1.532000\t2.749580\n",
    "1300\t1.577000\t2.743269\n",
    "1400\t1.519300\t2.733122\n",
    "1500\t1.300000\t2.726318\n",
    "1600\t1.361000\t2.719471\n",
    "1700\t1.414000\t2.710235\n",
    "1800\t1.323800\t2.706916\n",
    "1900\t1.304600\t2.694760\n",
    "2000\t1.357600\t2.688698\n",
    "2100\t1.428000\t2.684506\n",
    "2200\t1.390800\t2.679331\n",
    "2300\t1.331300\t2.673726\n",
    "2400\t1.402900\t2.667903\n",
    "2500\t1.466000\t2.666873\n",
    "2600\t1.396500\t2.659858\n",
    "2700\t1.285500\t2.655107\n",
    "2800\t1.336200\t2.651288\n",
    "2900\t1.479500\t2.648543\n",
    "3000\t1.530800\t2.644536\n",
    "3100\t1.337000\t2.639378\n",
    "3200\t1.358800\t2.636982\n",
    "3300\t1.310800\t2.633861\n",
    "3400\t1.246600\t2.630315\n",
    "3500\t1.269300\t2.627488\n",
    "3600\t1.354300\t2.624171\n",
    "3700\t1.267900\t2.623065\n",
    "3800\t1.558300\t2.620484\n",
    "3900\t1.497300\t2.617727\n",
    "4000\t1.275500\t2.615196\n",
    "4100\t1.433800\t2.613961\n",
    "4200\t1.317700\t2.610878\n",
    "4300\t1.270600\t2.609249\n",
    "4400\t1.395800\t2.608568\n",
    "4500\t1.359400\t2.606919\n",
    "4600\t1.453300\t2.605266\n",
    "4700\t1.354000\t2.604426\n",
    "4800\t1.250400\t2.602848\n",
    "4900\t1.480900\t2.602213\n",
    "5000\t1.406100\t2.600456\n",
    "5100\t1.386400\t2.599710\n",
    "5200\t1.433500\t2.599883\n",
    "5300\t1.526200\t2.599429\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b855d",
   "metadata": {},
   "source": [
    " Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa1d38",
   "metadata": {},
   "source": [
    "save_dir = \"./fine_tuned_dialoGPT\"\n",
    "\n",
    "import os, torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if os.path.exists(save_dir):\n",
    "    !rm -rf {save_dir}\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"Model saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278ef3d",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac86304",
   "metadata": {},
   "source": [
    "prompt = \"Tôi bị đau ngực và khó thở, có sao không?\"\n",
    "inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_length=100,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Chatbot test response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
