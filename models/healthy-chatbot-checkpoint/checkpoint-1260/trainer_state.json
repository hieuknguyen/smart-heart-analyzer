{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 1260,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 19.36109733581543,
      "learning_rate": 4.964285714285715e-05,
      "loss": 3.0484,
      "step": 10
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 13.779861450195312,
      "learning_rate": 4.924603174603175e-05,
      "loss": 2.0581,
      "step": 20
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 11.59946060180664,
      "learning_rate": 4.884920634920635e-05,
      "loss": 1.873,
      "step": 30
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 13.818879127502441,
      "learning_rate": 4.845238095238095e-05,
      "loss": 1.8096,
      "step": 40
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 12.785512924194336,
      "learning_rate": 4.805555555555556e-05,
      "loss": 1.7837,
      "step": 50
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 11.890185356140137,
      "learning_rate": 4.7658730158730166e-05,
      "loss": 1.7236,
      "step": 60
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 14.013275146484375,
      "learning_rate": 4.726190476190476e-05,
      "loss": 1.6569,
      "step": 70
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 12.59455680847168,
      "learning_rate": 4.686507936507937e-05,
      "loss": 1.6508,
      "step": 80
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 13.482123374938965,
      "learning_rate": 4.646825396825397e-05,
      "loss": 1.5443,
      "step": 90
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 12.212594985961914,
      "learning_rate": 4.607142857142857e-05,
      "loss": 1.6422,
      "step": 100
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 12.177019119262695,
      "learning_rate": 4.567460317460318e-05,
      "loss": 1.5629,
      "step": 110
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 14.255905151367188,
      "learning_rate": 4.527777777777778e-05,
      "loss": 1.5063,
      "step": 120
    },
    {
      "epoch": 1.0317460317460316,
      "grad_norm": 12.27763843536377,
      "learning_rate": 4.4880952380952385e-05,
      "loss": 1.3749,
      "step": 130
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 14.208858489990234,
      "learning_rate": 4.4484126984126986e-05,
      "loss": 1.4011,
      "step": 140
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 11.253132820129395,
      "learning_rate": 4.4087301587301587e-05,
      "loss": 1.3553,
      "step": 150
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 11.68507194519043,
      "learning_rate": 4.3690476190476194e-05,
      "loss": 1.2567,
      "step": 160
    },
    {
      "epoch": 1.3492063492063493,
      "grad_norm": 12.492583274841309,
      "learning_rate": 4.3293650793650795e-05,
      "loss": 1.3334,
      "step": 170
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 11.977706909179688,
      "learning_rate": 4.2896825396825396e-05,
      "loss": 1.3451,
      "step": 180
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 12.649415016174316,
      "learning_rate": 4.25e-05,
      "loss": 1.3642,
      "step": 190
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 12.137577056884766,
      "learning_rate": 4.2103174603174604e-05,
      "loss": 1.4152,
      "step": 200
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 10.863238334655762,
      "learning_rate": 4.170634920634921e-05,
      "loss": 1.3103,
      "step": 210
    },
    {
      "epoch": 1.746031746031746,
      "grad_norm": 11.97289752960205,
      "learning_rate": 4.130952380952381e-05,
      "loss": 1.2896,
      "step": 220
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 11.894112586975098,
      "learning_rate": 4.091269841269841e-05,
      "loss": 1.3029,
      "step": 230
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 11.779300689697266,
      "learning_rate": 4.051587301587302e-05,
      "loss": 1.1708,
      "step": 240
    },
    {
      "epoch": 1.9841269841269842,
      "grad_norm": 11.33535099029541,
      "learning_rate": 4.011904761904762e-05,
      "loss": 1.2252,
      "step": 250
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 11.013046264648438,
      "learning_rate": 3.972222222222222e-05,
      "loss": 1.0606,
      "step": 260
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 12.099782943725586,
      "learning_rate": 3.932539682539683e-05,
      "loss": 1.1009,
      "step": 270
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 10.65284252166748,
      "learning_rate": 3.892857142857143e-05,
      "loss": 1.0928,
      "step": 280
    },
    {
      "epoch": 2.3015873015873014,
      "grad_norm": 12.107216835021973,
      "learning_rate": 3.853174603174604e-05,
      "loss": 1.1223,
      "step": 290
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 12.04350471496582,
      "learning_rate": 3.813492063492063e-05,
      "loss": 1.1803,
      "step": 300
    },
    {
      "epoch": 2.4603174603174605,
      "grad_norm": 12.908478736877441,
      "learning_rate": 3.773809523809524e-05,
      "loss": 1.1678,
      "step": 310
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 12.383437156677246,
      "learning_rate": 3.7341269841269846e-05,
      "loss": 1.1646,
      "step": 320
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 12.301970481872559,
      "learning_rate": 3.694444444444445e-05,
      "loss": 1.1648,
      "step": 330
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 11.939908027648926,
      "learning_rate": 3.654761904761905e-05,
      "loss": 1.2137,
      "step": 340
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 11.051097869873047,
      "learning_rate": 3.615079365079365e-05,
      "loss": 1.0403,
      "step": 350
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 11.228702545166016,
      "learning_rate": 3.5753968253968256e-05,
      "loss": 1.1557,
      "step": 360
    },
    {
      "epoch": 2.9365079365079367,
      "grad_norm": 11.826081275939941,
      "learning_rate": 3.5357142857142864e-05,
      "loss": 1.0544,
      "step": 370
    },
    {
      "epoch": 3.015873015873016,
      "grad_norm": 11.019278526306152,
      "learning_rate": 3.496031746031746e-05,
      "loss": 1.1743,
      "step": 380
    },
    {
      "epoch": 3.0952380952380953,
      "grad_norm": 12.210628509521484,
      "learning_rate": 3.4563492063492065e-05,
      "loss": 1.0067,
      "step": 390
    },
    {
      "epoch": 3.1746031746031744,
      "grad_norm": 11.132369995117188,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 1.01,
      "step": 400
    },
    {
      "epoch": 3.253968253968254,
      "grad_norm": 12.025906562805176,
      "learning_rate": 3.3769841269841273e-05,
      "loss": 0.983,
      "step": 410
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 10.305718421936035,
      "learning_rate": 3.3373015873015874e-05,
      "loss": 0.98,
      "step": 420
    },
    {
      "epoch": 3.4126984126984126,
      "grad_norm": 11.998993873596191,
      "learning_rate": 3.2976190476190475e-05,
      "loss": 1.0577,
      "step": 430
    },
    {
      "epoch": 3.492063492063492,
      "grad_norm": 12.986720085144043,
      "learning_rate": 3.257936507936508e-05,
      "loss": 1.0071,
      "step": 440
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 11.675633430480957,
      "learning_rate": 3.218253968253968e-05,
      "loss": 1.0416,
      "step": 450
    },
    {
      "epoch": 3.6507936507936507,
      "grad_norm": 9.44143295288086,
      "learning_rate": 3.1785714285714284e-05,
      "loss": 1.0216,
      "step": 460
    },
    {
      "epoch": 3.7301587301587302,
      "grad_norm": 11.83267879486084,
      "learning_rate": 3.138888888888889e-05,
      "loss": 0.9941,
      "step": 470
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 9.556002616882324,
      "learning_rate": 3.099206349206349e-05,
      "loss": 0.988,
      "step": 480
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 12.089513778686523,
      "learning_rate": 3.05952380952381e-05,
      "loss": 1.0019,
      "step": 490
    },
    {
      "epoch": 3.9682539682539684,
      "grad_norm": 10.873069763183594,
      "learning_rate": 3.0198412698412697e-05,
      "loss": 1.0547,
      "step": 500
    },
    {
      "epoch": 4.0476190476190474,
      "grad_norm": 10.450491905212402,
      "learning_rate": 2.98015873015873e-05,
      "loss": 0.8817,
      "step": 510
    },
    {
      "epoch": 4.1269841269841265,
      "grad_norm": 10.629234313964844,
      "learning_rate": 2.940476190476191e-05,
      "loss": 0.9042,
      "step": 520
    },
    {
      "epoch": 4.2063492063492065,
      "grad_norm": 11.463438987731934,
      "learning_rate": 2.9007936507936513e-05,
      "loss": 0.8893,
      "step": 530
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 12.148397445678711,
      "learning_rate": 2.861111111111111e-05,
      "loss": 0.9785,
      "step": 540
    },
    {
      "epoch": 4.365079365079365,
      "grad_norm": 11.2890043258667,
      "learning_rate": 2.8214285714285714e-05,
      "loss": 0.8856,
      "step": 550
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 11.190032958984375,
      "learning_rate": 2.781746031746032e-05,
      "loss": 0.8763,
      "step": 560
    },
    {
      "epoch": 4.523809523809524,
      "grad_norm": 11.435379981994629,
      "learning_rate": 2.7420634920634926e-05,
      "loss": 0.9037,
      "step": 570
    },
    {
      "epoch": 4.603174603174603,
      "grad_norm": 9.28894329071045,
      "learning_rate": 2.7023809523809523e-05,
      "loss": 0.9067,
      "step": 580
    },
    {
      "epoch": 4.682539682539683,
      "grad_norm": 11.66087818145752,
      "learning_rate": 2.6626984126984127e-05,
      "loss": 0.9566,
      "step": 590
    },
    {
      "epoch": 4.761904761904762,
      "grad_norm": 10.937475204467773,
      "learning_rate": 2.623015873015873e-05,
      "loss": 0.9539,
      "step": 600
    },
    {
      "epoch": 4.841269841269841,
      "grad_norm": 11.27419662475586,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.9162,
      "step": 610
    },
    {
      "epoch": 4.920634920634921,
      "grad_norm": 9.932462692260742,
      "learning_rate": 2.5436507936507936e-05,
      "loss": 0.912,
      "step": 620
    },
    {
      "epoch": 5.0,
      "grad_norm": 15.413487434387207,
      "learning_rate": 2.503968253968254e-05,
      "loss": 0.8634,
      "step": 630
    },
    {
      "epoch": 5.079365079365079,
      "grad_norm": 9.839451789855957,
      "learning_rate": 2.4642857142857145e-05,
      "loss": 0.7617,
      "step": 640
    },
    {
      "epoch": 5.158730158730159,
      "grad_norm": 11.189253807067871,
      "learning_rate": 2.424603174603175e-05,
      "loss": 0.8828,
      "step": 650
    },
    {
      "epoch": 5.238095238095238,
      "grad_norm": 11.236061096191406,
      "learning_rate": 2.3849206349206353e-05,
      "loss": 0.8495,
      "step": 660
    },
    {
      "epoch": 5.317460317460317,
      "grad_norm": 10.909292221069336,
      "learning_rate": 2.3452380952380954e-05,
      "loss": 0.8675,
      "step": 670
    },
    {
      "epoch": 5.396825396825397,
      "grad_norm": 9.519835472106934,
      "learning_rate": 2.3055555555555558e-05,
      "loss": 0.8344,
      "step": 680
    },
    {
      "epoch": 5.476190476190476,
      "grad_norm": 11.297524452209473,
      "learning_rate": 2.265873015873016e-05,
      "loss": 0.8155,
      "step": 690
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 8.321136474609375,
      "learning_rate": 2.2261904761904763e-05,
      "loss": 0.8089,
      "step": 700
    },
    {
      "epoch": 5.634920634920634,
      "grad_norm": 10.765466690063477,
      "learning_rate": 2.1865079365079367e-05,
      "loss": 0.8733,
      "step": 710
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 9.452912330627441,
      "learning_rate": 2.1468253968253967e-05,
      "loss": 0.801,
      "step": 720
    },
    {
      "epoch": 5.7936507936507935,
      "grad_norm": 10.607257843017578,
      "learning_rate": 2.107142857142857e-05,
      "loss": 0.8653,
      "step": 730
    },
    {
      "epoch": 5.8730158730158735,
      "grad_norm": 12.866761207580566,
      "learning_rate": 2.0674603174603176e-05,
      "loss": 0.883,
      "step": 740
    },
    {
      "epoch": 5.9523809523809526,
      "grad_norm": 9.407083511352539,
      "learning_rate": 2.027777777777778e-05,
      "loss": 0.8586,
      "step": 750
    },
    {
      "epoch": 6.031746031746032,
      "grad_norm": 10.867757797241211,
      "learning_rate": 1.988095238095238e-05,
      "loss": 0.7691,
      "step": 760
    },
    {
      "epoch": 6.111111111111111,
      "grad_norm": 10.531978607177734,
      "learning_rate": 1.9484126984126985e-05,
      "loss": 0.7516,
      "step": 770
    },
    {
      "epoch": 6.190476190476191,
      "grad_norm": 9.614152908325195,
      "learning_rate": 1.9087301587301585e-05,
      "loss": 0.7622,
      "step": 780
    },
    {
      "epoch": 6.26984126984127,
      "grad_norm": 9.119525909423828,
      "learning_rate": 1.8690476190476193e-05,
      "loss": 0.7268,
      "step": 790
    },
    {
      "epoch": 6.349206349206349,
      "grad_norm": 9.364541053771973,
      "learning_rate": 1.8293650793650794e-05,
      "loss": 0.773,
      "step": 800
    },
    {
      "epoch": 6.428571428571429,
      "grad_norm": 10.374176979064941,
      "learning_rate": 1.7896825396825398e-05,
      "loss": 0.8018,
      "step": 810
    },
    {
      "epoch": 6.507936507936508,
      "grad_norm": 9.824695587158203,
      "learning_rate": 1.75e-05,
      "loss": 0.7747,
      "step": 820
    },
    {
      "epoch": 6.587301587301587,
      "grad_norm": 9.196722984313965,
      "learning_rate": 1.7103174603174606e-05,
      "loss": 0.7525,
      "step": 830
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 10.661343574523926,
      "learning_rate": 1.6706349206349207e-05,
      "loss": 0.7589,
      "step": 840
    },
    {
      "epoch": 6.746031746031746,
      "grad_norm": 10.740265846252441,
      "learning_rate": 1.630952380952381e-05,
      "loss": 0.8313,
      "step": 850
    },
    {
      "epoch": 6.825396825396825,
      "grad_norm": 9.912747383117676,
      "learning_rate": 1.591269841269841e-05,
      "loss": 0.7857,
      "step": 860
    },
    {
      "epoch": 6.904761904761905,
      "grad_norm": 11.437507629394531,
      "learning_rate": 1.5515873015873016e-05,
      "loss": 0.7936,
      "step": 870
    },
    {
      "epoch": 6.984126984126984,
      "grad_norm": 9.414593696594238,
      "learning_rate": 1.5119047619047618e-05,
      "loss": 0.7771,
      "step": 880
    },
    {
      "epoch": 7.063492063492063,
      "grad_norm": 9.644932746887207,
      "learning_rate": 1.4722222222222224e-05,
      "loss": 0.7215,
      "step": 890
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 11.698549270629883,
      "learning_rate": 1.4325396825396825e-05,
      "loss": 0.7108,
      "step": 900
    },
    {
      "epoch": 7.222222222222222,
      "grad_norm": 11.435895919799805,
      "learning_rate": 1.392857142857143e-05,
      "loss": 0.7119,
      "step": 910
    },
    {
      "epoch": 7.301587301587301,
      "grad_norm": 10.178094863891602,
      "learning_rate": 1.3531746031746031e-05,
      "loss": 0.7155,
      "step": 920
    },
    {
      "epoch": 7.380952380952381,
      "grad_norm": 8.698397636413574,
      "learning_rate": 1.3134920634920635e-05,
      "loss": 0.7035,
      "step": 930
    },
    {
      "epoch": 7.4603174603174605,
      "grad_norm": 12.685749053955078,
      "learning_rate": 1.2738095238095238e-05,
      "loss": 0.7648,
      "step": 940
    },
    {
      "epoch": 7.5396825396825395,
      "grad_norm": 12.75523567199707,
      "learning_rate": 1.2341269841269842e-05,
      "loss": 0.7123,
      "step": 950
    },
    {
      "epoch": 7.619047619047619,
      "grad_norm": 11.48247241973877,
      "learning_rate": 1.1944444444444446e-05,
      "loss": 0.7892,
      "step": 960
    },
    {
      "epoch": 7.698412698412699,
      "grad_norm": 9.980151176452637,
      "learning_rate": 1.1547619047619048e-05,
      "loss": 0.6977,
      "step": 970
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 9.734123229980469,
      "learning_rate": 1.1150793650793653e-05,
      "loss": 0.7499,
      "step": 980
    },
    {
      "epoch": 7.857142857142857,
      "grad_norm": 9.363786697387695,
      "learning_rate": 1.0753968253968255e-05,
      "loss": 0.7402,
      "step": 990
    },
    {
      "epoch": 7.936507936507937,
      "grad_norm": 10.362614631652832,
      "learning_rate": 1.0357142857142859e-05,
      "loss": 0.7456,
      "step": 1000
    },
    {
      "epoch": 8.015873015873016,
      "grad_norm": 9.323417663574219,
      "learning_rate": 9.96031746031746e-06,
      "loss": 0.7263,
      "step": 1010
    },
    {
      "epoch": 8.095238095238095,
      "grad_norm": 9.032196044921875,
      "learning_rate": 9.563492063492064e-06,
      "loss": 0.6729,
      "step": 1020
    },
    {
      "epoch": 8.174603174603174,
      "grad_norm": 9.34683609008789,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.6964,
      "step": 1030
    },
    {
      "epoch": 8.253968253968253,
      "grad_norm": 10.471175193786621,
      "learning_rate": 8.769841269841269e-06,
      "loss": 0.7273,
      "step": 1040
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 9.295003890991211,
      "learning_rate": 8.373015873015873e-06,
      "loss": 0.7045,
      "step": 1050
    },
    {
      "epoch": 8.412698412698413,
      "grad_norm": 10.340864181518555,
      "learning_rate": 7.976190476190475e-06,
      "loss": 0.6933,
      "step": 1060
    },
    {
      "epoch": 8.492063492063492,
      "grad_norm": 10.332320213317871,
      "learning_rate": 7.5793650793650795e-06,
      "loss": 0.7197,
      "step": 1070
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 10.094830513000488,
      "learning_rate": 7.182539682539682e-06,
      "loss": 0.7164,
      "step": 1080
    },
    {
      "epoch": 8.65079365079365,
      "grad_norm": 9.942889213562012,
      "learning_rate": 6.785714285714285e-06,
      "loss": 0.7018,
      "step": 1090
    },
    {
      "epoch": 8.73015873015873,
      "grad_norm": 8.826889038085938,
      "learning_rate": 6.3888888888888885e-06,
      "loss": 0.675,
      "step": 1100
    },
    {
      "epoch": 8.80952380952381,
      "grad_norm": 10.469993591308594,
      "learning_rate": 5.992063492063493e-06,
      "loss": 0.6862,
      "step": 1110
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 9.226194381713867,
      "learning_rate": 5.595238095238096e-06,
      "loss": 0.7244,
      "step": 1120
    },
    {
      "epoch": 8.968253968253968,
      "grad_norm": 7.7871012687683105,
      "learning_rate": 5.198412698412699e-06,
      "loss": 0.7027,
      "step": 1130
    },
    {
      "epoch": 9.047619047619047,
      "grad_norm": 9.94082260131836,
      "learning_rate": 4.8015873015873025e-06,
      "loss": 0.6253,
      "step": 1140
    },
    {
      "epoch": 9.126984126984127,
      "grad_norm": 8.781970024108887,
      "learning_rate": 4.404761904761905e-06,
      "loss": 0.6478,
      "step": 1150
    },
    {
      "epoch": 9.206349206349206,
      "grad_norm": 9.206480026245117,
      "learning_rate": 4.007936507936508e-06,
      "loss": 0.6625,
      "step": 1160
    },
    {
      "epoch": 9.285714285714286,
      "grad_norm": 8.332894325256348,
      "learning_rate": 3.611111111111111e-06,
      "loss": 0.6488,
      "step": 1170
    },
    {
      "epoch": 9.365079365079366,
      "grad_norm": 11.350059509277344,
      "learning_rate": 3.2142857142857143e-06,
      "loss": 0.687,
      "step": 1180
    },
    {
      "epoch": 9.444444444444445,
      "grad_norm": 9.582737922668457,
      "learning_rate": 2.8174603174603176e-06,
      "loss": 0.6668,
      "step": 1190
    },
    {
      "epoch": 9.523809523809524,
      "grad_norm": 9.074761390686035,
      "learning_rate": 2.420634920634921e-06,
      "loss": 0.6738,
      "step": 1200
    },
    {
      "epoch": 9.603174603174603,
      "grad_norm": 9.212890625,
      "learning_rate": 2.023809523809524e-06,
      "loss": 0.6843,
      "step": 1210
    },
    {
      "epoch": 9.682539682539682,
      "grad_norm": 9.260805130004883,
      "learning_rate": 1.6269841269841272e-06,
      "loss": 0.6297,
      "step": 1220
    },
    {
      "epoch": 9.761904761904763,
      "grad_norm": 8.850778579711914,
      "learning_rate": 1.2301587301587303e-06,
      "loss": 0.7531,
      "step": 1230
    },
    {
      "epoch": 9.841269841269842,
      "grad_norm": 8.89234733581543,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.6762,
      "step": 1240
    },
    {
      "epoch": 9.920634920634921,
      "grad_norm": 9.467222213745117,
      "learning_rate": 4.3650793650793655e-07,
      "loss": 0.6746,
      "step": 1250
    },
    {
      "epoch": 10.0,
      "grad_norm": 12.588266372680664,
      "learning_rate": 3.9682539682539686e-08,
      "loss": 0.6682,
      "step": 1260
    }
  ],
  "logging_steps": 10,
  "max_steps": 1260,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 655843000320000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
